{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGB7lsKwglWRj876nlxfMf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/majd-adawieh/multi-agent-rl/blob/main/multi_agent_pong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMdVyAoTjRko"
      },
      "outputs": [],
      "source": [
        "!pip install pettingzoo\n",
        "!pip install pettingzoo[all]\n",
        "!pip install supersuit\n",
        "!pip install multi-agent-ale-py\n",
        "!pip install ale-py\n",
        "!pip install AutoROM\n",
        "!pip install pyvirtualdisplay pytorch-lightning\n",
        "!AutoROM -y\n",
        "!apt-get install -y xvfb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import copy\n",
        "from collections import deque\n",
        "import itertools\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from supersuit import resize_v1, color_reduction_v0, normalize_obs_v0, reshape_v0, frame_skip_v0\n",
        "from pettingzoo.atari import pong_v3\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, IterableDataset\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "\n",
        "Display(visible=False, size=(1400, 900)).start()\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ],
      "metadata": {
        "id": "PeFbp7gVnQMl"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb#scrollTo=gKc1FNhKiVJX\n",
        "def display_video(frames, framerate=30):\n",
        "  height, width, _ = frames[0].shape\n",
        "  dpi = 70\n",
        "  orig_backend = matplotlib.get_backend()\n",
        "  matplotlib.use('Agg')\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
        "  matplotlib.use(orig_backend)\n",
        "  ax.set_axis_off()\n",
        "  ax.set_aspect('equal')\n",
        "  ax.set_position([0, 0, 1, 1])\n",
        "  im = ax.imshow(frames[0])\n",
        "  def update(frame):\n",
        "    im.set_data(frame)\n",
        "    return [im]\n",
        "  interval = 1000/framerate\n",
        "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                  interval=interval, blit=True, repeat=False)\n",
        "  return HTML(anim.to_html5_video())"
      ],
      "metadata": {
        "id": "GHC4e2p5lSHz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_env(num_agents):\n",
        "  env = pong_v3.env(num_players=num_agents, max_cycles=900)\n",
        "  env = frame_skip_v0(env, 8)\n",
        "  env = resize_v1(env, x_size=64, y_size=64)\n",
        "  #env = color_reduction_v0(env, 'full')\n",
        "  #env = reshape_v0(env, (3,64,64)) \n",
        "  return env\n",
        "\n",
        "def normalize_observations(o):\n",
        "  o = Image.fromarray(o).convert(\"L\") # Remolve color\n",
        "  o = np.asarray(o)\n",
        "  o = np.expand_dims(o, axis=0)\n",
        "  o = np.interp(o, (o.min(), o.max()), (0, +1))\n",
        "  return torch.from_numpy(o)\n",
        "\n",
        "\n",
        "env = create_env(2)\n",
        "env.reset()\n",
        "o = env.observe(\"first_0\")\n",
        "\n",
        "print(o.shape)\n",
        "o = normalize_observations(o)\n",
        "print(o.shape)\n"
      ],
      "metadata": {
        "id": "trwaYkSt7jw1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f0a681a-d286-408f-b06b-2f386432db6c"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 64, 3)\n",
            "torch.Size([1, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "    \n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "        \n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)"
      ],
      "metadata": {
        "id": "hJmJ6-qdarH4"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RLDataset(IterableDataset):\n",
        "    def __init__(self, buffer, sample_size=400):\n",
        "        self.buffer = buffer\n",
        "        self.sample_size = sample_size\n",
        "    \n",
        "    def __iter__(self):\n",
        "        for experience in self.buffer.sample(self.sample_size):\n",
        "            yield experience"
      ],
      "metadata": {
        "id": "ttUGSxKlbh4Y"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(state, env, net, epsilon=0.0):\n",
        "    if np.random.random() < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        state = state.to(device)\n",
        "        q_values = net(state)\n",
        "        _, action = torch.max(q_values, dim=1)\n",
        "        action = int(action.item())\n",
        "    return action"
      ],
      "metadata": {
        "id": "PVR51pCNnEQv"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size , n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        \n",
        "        self.state_size = state_size\n",
        "        self.conv = nn.Sequential(\n",
        "                        nn.Conv2d(state_size[0], 32, 3, stride=2, padding=1),\n",
        "                        nn.ELU(),\n",
        "                        nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
        "                        nn.ELU(),\n",
        "                        nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
        "                        nn.ELU(),\n",
        "                        nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
        "                        nn.ELU()\n",
        "                    )\n",
        "        conv_out_size = self._get_conv_out(state_size)\n",
        "        self.fc1 = nn.Linear(conv_out_size, 256)\n",
        "        self.fc_adv = nn.Linear(256, n_actions) \n",
        "        self.fc_value = nn.Linear(256, 1)\n",
        "        \n",
        "    def _get_conv_out(self, shape):\n",
        "        conv_out = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(conv_out.size()))\n",
        "    \n",
        "    def forward(self, x):        \n",
        "        o = self.conv(x.float()).view(x.shape[0], -1)\n",
        "        o = F.relu(self.fc1(o))\n",
        "        \n",
        "        adv = self.fc_adv(o)\n",
        "        value = self.fc_value(o)  \n",
        "        \n",
        "        return value + adv - torch.mean(adv, dim=1, keepdim=True)"
      ],
      "metadata": {
        "id": "VLKxI_OynWeO"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yKk4sqMrz3NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepQLearning(LightningModule):\n",
        "    def __init__(self, policy=epsilon_greedy, capacity=100_000, \n",
        "               batch_size=256, lr=1e-3, hidden_size=128, gamma=0.99, \n",
        "               loss_fn=nn.MSELoss(), optim=AdamW, eps_start=1.0, eps_end=0.15, \n",
        "               eps_last_episode=400, samples_per_epoch=1024, sync_rate=10,\n",
        "               sequence_length = 8):\n",
        "    \n",
        "        super().__init__()\n",
        "        self.env = create_env(2)\n",
        "\n",
        "        obs_size = (1, 64, 64)\n",
        "        n_actions = 6\n",
        "\n",
        "        self.q_net = DQN(obs_size, n_actions)\n",
        "\n",
        "        self.target_q_net = copy.deepcopy(self.q_net)\n",
        "\n",
        "        self.policy = policy\n",
        "        self.buffer = ReplayBuffer(capacity=capacity)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "            print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "            self.play_episode(epsilon=self.hparams.eps_start)\n",
        "\n",
        "\n",
        "        \n",
        "    @torch.no_grad()\n",
        "    def play_episode(self, policy=None, epsilon=0.):\n",
        "        self.env.reset()\n",
        "        done = False\n",
        "        last_observation = self.env.observe(self.env.agent_selection)\n",
        "        last_action = 0\n",
        "        returns = 0\n",
        "        frames = []\n",
        "        while not done:\n",
        "          current_agent = self.env.agent_selection\n",
        "          if current_agent == \"first_0\":\n",
        "            new_observation, last_reward, done, truncated, _ = self.env.last()\n",
        "            returns += last_reward\n",
        "            frames.append(new_observation)\n",
        "            state = normalize_observations(new_observation)\n",
        "            if truncated:\n",
        "              done = truncated\n",
        "            \n",
        "            if not done:\n",
        "              if policy:\n",
        "                  action = policy(state.unsqueeze(dim=0), self.env, self.q_net, epsilon=epsilon)\n",
        "              else:\n",
        "                  action = random.randint(0,5)\n",
        "\n",
        "              self.env.step(action)\n",
        "              exp = (last_observation, last_action, last_reward, done, new_observation)\n",
        "              self.buffer.append(exp)\n",
        "              last_action = action\n",
        "              last_observation = new_observation\n",
        "          else:\n",
        "            action = random.randint(0,5)\n",
        "            self.env.step(action)\n",
        "\n",
        "        self.logger(\"episode/Return\",returns)\n",
        "        if self.current_epoch % 100 == 0:\n",
        "          video = display_video(frames)\n",
        "          with open(\"video\"+str(self.current_epoch)+\".html\", \"r+\") as file1:\n",
        "            file1.write(video.data)\n",
        "\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.q_net(x)\n",
        "\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n",
        "        return [q_net_optimizer]\n",
        "\n",
        "     # Create dataloader.\n",
        "    def train_dataloader(self):\n",
        "        dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "       \n",
        "        dataloader = DataLoader(\n",
        "            dataset=dataset,\n",
        "            batch_size=self.hparams.batch_size\n",
        "        )\n",
        "        return dataloader\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        states, actions, rewards, dones, next_states = batch\n",
        "        actions = actions.unsqueeze(1)\n",
        "        rewards = rewards.unsqueeze(1)\n",
        "        dones = dones.unsqueeze(1)\n",
        "        \n",
        "        state_action_values = self.q_net(states).gather(1, actions)\n",
        "\n",
        "        next_action_values, _ = self.target_q_net(next_states).max(dim=1, keepdim=True)\n",
        "        next_action_values[dones] = 0.0\n",
        "\n",
        "        expected_state_action_values = rewards + self.hparams.gamma * next_action_values\n",
        "\n",
        "        loss = self.hparams.loss_fn(state_action_values.float(), expected_state_action_values.float())\n",
        "        self.log('episode/Q-Error', loss)\n",
        "        return loss\n",
        "    \n",
        "    # Training epoch end.\n",
        "    def training_epoch_end(self, training_step_outputs):\n",
        "        epsilon = max(\n",
        "            self.hparams.eps_end,\n",
        "            self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
        "        )\n",
        "\n",
        "        self.play_episode(policy=self.policy, epsilon=epsilon)\n",
        "\n",
        "        if self.current_epoch % self.hparams.sync_rate == 0:\n",
        "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
        "            \n",
        "            \n",
        "    def save_model(self):\n",
        "        torch.save(self.q_net.state_dict(), \"./model\")\n",
        "        \n",
        "    def load_model(self):\n",
        "        self.q_net.load_state_dict(torch.load( \"./model\"))\n"
      ],
      "metadata": {
        "id": "OflFw6LqdFNb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}