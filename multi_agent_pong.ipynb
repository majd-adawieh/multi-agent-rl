{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFQvjlDCzbGJ4E3hWUKqKl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/majd-adawieh/multi-agent-rl/blob/main/multi_agent_pong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMdVyAoTjRko"
      },
      "outputs": [],
      "source": [
        "!pip install pettingzoo\n",
        "!pip install pettingzoo[all]\n",
        "!pip install supersuit\n",
        "!pip install multi-agent-ale-py\n",
        "!pip install ale-py\n",
        "!pip install AutoROM\n",
        "!pip install pyvirtualdisplay pytorch-lightning\n",
        "!AutoROM -y\n",
        "!apt-get install -y xvfb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import copy\n",
        "from collections import deque\n",
        "import itertools\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from supersuit import resize_v1, color_reduction_v0, normalize_obs_v0, reshape_v0, frame_skip_v0\n",
        "from pettingzoo.atari import pong_v3\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, IterableDataset\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "\n",
        "Display(visible=False, size=(1400, 900)).start()\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ],
      "metadata": {
        "id": "PeFbp7gVnQMl"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb#scrollTo=gKc1FNhKiVJX\n",
        "def display_video(frames, framerate=30):\n",
        "  height, width, _ = frames[0].shape\n",
        "  dpi = 70\n",
        "  orig_backend = matplotlib.get_backend()\n",
        "  matplotlib.use('Agg')\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
        "  matplotlib.use(orig_backend)\n",
        "  ax.set_axis_off()\n",
        "  ax.set_aspect('equal')\n",
        "  ax.set_position([0, 0, 1, 1])\n",
        "  im = ax.imshow(frames[0])\n",
        "  def update(frame):\n",
        "    im.set_data(frame)\n",
        "    return [im]\n",
        "  interval = 1000/framerate\n",
        "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                  interval=interval, blit=True, repeat=False)\n",
        "  return HTML(anim.to_html5_video())"
      ],
      "metadata": {
        "id": "GHC4e2p5lSHz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_env(num_agents):\n",
        "  env = pong_v3.env(num_players=num_agents, max_cycles=900)\n",
        "  env = frame_skip_v0(env, 8)\n",
        "  env = resize_v1(env, x_size=64, y_size=64)\n",
        "  #env = color_reduction_v0(env, 'full')\n",
        "  env = reshape_v0(env, (3,64,64)) \n",
        "  return env\n",
        "\n",
        "def normalize_observations(o):\n",
        "  return np.interp(o, (o.min(), o.max()), (0, +1))\n",
        "\n",
        "\n",
        "env = create_env(2)\n",
        "\n",
        "frames = []\n",
        "\n",
        "def play_episode():\n",
        "  env.reset()\n",
        "  done = False  \n",
        "  last_observation = env.observe(env.agent_selection)\n",
        "  last_action = 0\n",
        "  while not done:\n",
        "    current_agent = env.agent_selection\n",
        "    if current_agent == \"first_0\":\n",
        "      new_observation, last_reward, done, truncated, _ = env.last()\n",
        "      frames.append(new_observation.reshape((64,64,3)))\n",
        "      if truncated:\n",
        "        done = truncated\n",
        "      action = random.randint(0,5)\n",
        "      if not done:\n",
        "        env.step(action)\n",
        "        exp = (last_observation, last_action, last_reward, done, new_observation)\n",
        "        print(f\"last_action {last_action}, last_reward {last_reward}\")\n",
        "        last_action = action\n",
        "        last_observation = new_observation\n",
        "    else:\n",
        "      action = random.randint(0,5) # random\n",
        "      env.step(action)\n",
        "\n",
        "\n",
        "play_episode()\n",
        "\n"
      ],
      "metadata": {
        "id": "trwaYkSt7jw1",
        "outputId": "69723948-045d-4612-89ac-d226a075982b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "last_action 0, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 3, last_reward 0.0\n",
            "last_action 3, last_reward 0.0\n",
            "last_action 3, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 3, last_reward 0.0\n",
            "last_action 1, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 1, last_reward 0.0\n",
            "last_action 3, last_reward 0.0\n",
            "last_action 3, last_reward 0.0\n",
            "last_action 3, last_reward 0.0\n",
            "last_action 0, last_reward 1.0\n",
            "last_action 3, last_reward 0.0\n",
            "last_action 1, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 2, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 2, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 2, last_reward 0.0\n",
            "last_action 2, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 1, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 2, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 2, last_reward 0.0\n",
            "last_action 1, last_reward 0.0\n",
            "last_action 2, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 1, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 1, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 1, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 3, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 1, last_reward 0.0\n",
            "last_action 3, last_reward 0.0\n",
            "last_action 4, last_reward -1.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 1, last_reward 0.0\n",
            "last_action 1, last_reward 0.0\n",
            "last_action 2, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 2, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 2, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 3, last_reward 0.0\n",
            "last_action 1, last_reward 0.0\n",
            "last_action 0, last_reward -1.0\n",
            "last_action 2, last_reward 0.0\n",
            "last_action 1, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 1, last_reward 0.0\n",
            "last_action 3, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 1, last_reward 0.0\n",
            "last_action 3, last_reward 0.0\n",
            "last_action 3, last_reward 0.0\n",
            "last_action 2, last_reward 0.0\n",
            "last_action 2, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 3, last_reward -1.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 1, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 0, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 2, last_reward 0.0\n",
            "last_action 2, last_reward 0.0\n",
            "last_action 1, last_reward -1.0\n",
            "last_action 5, last_reward 0.0\n",
            "last_action 4, last_reward 0.0\n",
            "last_action 3, last_reward 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_video(frames)"
      ],
      "metadata": {
        "id": "CxLj6MLxttST",
        "outputId": "bf3bf61b-fd7b-47a6-f0a2-a066d038ed09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=\"64\" height=\"64\" controls autoplay>\n",
              "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAHGZ0eXBNNFYgAAACAGlzb21pc28yYXZjMQAAAAhmcmVlAAAYWW1kYXQAAAKuBgX//6rcRem9\n",
              "5tlIt5Ys2CDZI+7veDI2NCAtIGNvcmUgMTUyIHIyODU0IGU5YTU5MDMgLSBILjI2NC9NUEVHLTQg\n",
              "QVZDIGNvZGVjIC0gQ29weWxlZnQgMjAwMy0yMDE3IC0gaHR0cDovL3d3dy52aWRlb2xhbi5vcmcv\n",
              "eDI2NC5odG1sIC0gb3B0aW9uczogY2FiYWM9MSByZWY9MyBkZWJsb2NrPTE6MDowIGFuYWx5c2U9\n",
              "MHgzOjB4MTEzIG1lPWhleCBzdWJtZT03IHBzeT0xIHBzeV9yZD0xLjAwOjAuMDAgbWl4ZWRfcmVm\n",
              "PTEgbWVfcmFuZ2U9MTYgY2hyb21hX21lPTEgdHJlbGxpcz0xIDh4OGRjdD0xIGNxbT0wIGRlYWR6\n",
              "b25lPTIxLDExIGZhc3RfcHNraXA9MSBjaHJvbWFfcXBfb2Zmc2V0PS0yIHRocmVhZHM9MiBsb29r\n",
              "YWhlYWRfdGhyZWFkcz0xIHNsaWNlZF90aHJlYWRzPTAgbnI9MCBkZWNpbWF0ZT0xIGludGVybGFj\n",
              "ZWQ9MCBibHVyYXlfY29tcGF0PTAgY29uc3RyYWluZWRfaW50cmE9MCBiZnJhbWVzPTMgYl9weXJh\n",
              "bWlkPTIgYl9hZGFwdD0xIGJfYmlhcz0wIGRpcmVjdD0xIHdlaWdodGI9MSBvcGVuX2dvcD0wIHdl\n",
              "aWdodHA9MiBrZXlpbnQ9MjUwIGtleWludF9taW49MjUgc2NlbmVjdXQ9NDAgaW50cmFfcmVmcmVz\n",
              "aD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42MCBx\n",
              "cG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAADfZYiE\n",
              "AEfpKFMCf/euSl0VwmfOu/ZrG0RRFZRsNxpkOvDNqQhzxUDYYtfwoMXp14KqAam3rq5Db5Jalg6J\n",
              "h3taoQMu6hV903Pced9REh2a7jdTNV/js3JM3Ywj//qMIKeSA5yqUl7eSC2zd6QfdjqlQx0kvJfW\n",
              "cU6aUpQ5hnUcNYlu4lY6CCAyD3wbNOFkauYnfPHhtJN35BpkjCBfSlose7lwwj6Pct7IVIiH8b2X\n",
              "lHuLzs4eHPKt2FeQ4KJE7W9D3/Nv5uweW6NwmbuwJI8wdqOCebc6/hTsn2TbqU77wQAAACtBmiJs\n",
              "RX+99pd0W3j8oQ/7k6adz3F1+cWAswY3oB+IlC4fot4dUXtgZSuUAAAAJQGeQXkLf/i5IdTB8YT+\n",
              "EbmZx8mODwbDzbORkvVSCL/5GO5elqkAAAARQZpDPCGTKYRXBSSVkccgifgAAAAUQZpkSeEPJlMC\n",
              "K/+7Yb/onU8irb8AAAAlQZqFSeEPJlMCK/+7YcAjJu6UbfgR0Wm/Q/MKR9Y6U/aW34yMpQAAANZB\n",
              "iKmCf/JMJSP8QN8nTLY0Qdl1++faSzCbTNAVhYZ+XhA7RUgr6gl9w/OOdImJ32slkI04FX2Wxmt/\n",
              "+j1Bv3rYUJzayr7ch7pdaNc2Ad+ImMz32tbs5/eM353Lot2ytXu1r4VIyrP4K1jMtHXufVEGx6sP\n",
              "sGsy/9d3WvL6NlZlzkOOfGPzBKCi/GnQ+kzLxFIWtCsWW4de+gIp//yvGwF4EThw79Fi4AcoLjUu\n",
              "uZ4A3i4Gz5a1PxhudjwSUMJFxXCVJEHEh/ZCU7/qaulwpz+RjaTI9mhxAAAAOUGayknhDyZTAr/G\n",
              "NAZRymNKUsvhI53Wu22uCPv6LYd1GKf57pcjlHMGOgSxRuzBnlpA4jkdSWppFwAAACFBnuhFETwt\n",
              "//70zjA7mK/9NGsC//v3Vizw/DyV7qIqrsAAAAARAZ8HdELf+L5AcC9Tc5KFCIAAAAAnAZ8JakLf\n",
              "8KpbTs5BeraDVWqRV3GVRYH/P3Vuqu0lyXdV4e0q+iL1AAAA1EGIwsf/eHUXwr/AwxBpVGwXRpL+\n",
              "bv54uERlj6cvKJzE5fuBJyICL/xI2h1tvX5VIDHXnPd7WV6rHNIZSIqV65NH6QEi3GWbvCaHiLee\n",
              "CSmmjPf23BVr3daAa/iTuE6HEchfpT9BJj5ZA+ry3fysyoM7mIrbvDM3NmTC2TUVObUMkzwKQk4N\n",
              "7sKFZWYbxmBbGARUuk4Mpug3Hscr35tZk02+JBhOoWW/9jC2OSkKQYHa8RycBZ++lhIikWwOVe37\n",
              "/FJFrnjHcoTWiGlkDBXWihHpLE2AAAAAD0GbLEnhClJlMCv/AXSnMgAAACVBm1BJ4Q6JlMCT/x1m\n",
              "LgPvGw7ziL4O0ww/3bZLG5bwstm2sw9lAAAAIUGfbkURPCX/tmF6FCk6zBxQQkIwmH0CIpIisZ4/\n",
              "dGK4oQAAABQBn410Qt+8imssCRo0ldT3mstzgQAAABcBn49qQt+8aNcja3H/6G2br1MKJKxnKAAA\n",
              "ACVBm5NJqEFomUwJP2N3WXDfuXrjeQ5bsBr4H8vvPde1cMdTz6nAAAAAEUGfsUURLC3/8UVJUzj/\n",
              "VQWxAAAADQGf0mpC3/DGgOyUE1AAAAAkQZvUSahBbJlMCT8gPT9RDHvAigsUg8Yc/mHdpY1syaJi\n",
              "4UUgAAAAt0GI/UX/7uYl1t+ZChcKr5OTp1AB/QEZzb/HGZ8+3WgrQ0Vwh/RRgAWapytMjW+Sh5Tw\n",
              "VfPT5kg4YyJEZ5SLtwiJRffnbYD70zXHaBBySsTBY4Bki2u+3NW4lfI+3Xb/oEdALrfIGG0vv3Ac\n",
              "vgrVHXmg8WoUgjAFa/sS1Ny6NqpCFt0yGcpYwDVLmAW8QE6lIAY675lS0S1kb/AyxF840CCdI1IA\n",
              "kGalmY7GN80a/kGR0Y+aiNt7wQAAADRBmhlJ4Q6JlMCK/9ek/IUzbx+UIf9ydNPDLkCbWi3V4TNp\n",
              "d6z7TfttOW5XrmBLRmRlOJncAAAAHkGeN0URPCH/+g6zqQBBPHgXpH1qLs1vtIOtLboy4QAAACYB\n",
              "nlZ0Qt/AurPIc9K/wanmVfhaSFliULshUlkLq13Qa6qL0sPJgQAAAA4BnlhqQt/4ZFVUm8ZIkAAA\n",
              "ABZBml1JqEFomUwIr9ek/IUzbx+X71+hAAAAC0Gee0URLCH/7OXcAAAACQGemnRC3/AACQAAAAgB\n",
              "npxqQt9pwQAAABhBmp9JqEFsmUwUTGc1CQOG3d5Gf4c1B4AAAAATAZ6+akLfsTBwNSnv+KbkoGP7\n",
              "nQAAACpBmqBJ4QpSZTAiv0YRoio4UEp4cWbNOTc1HQawOg2BdPhaTbIeO1cTpFcAAADQZYiCADfm\n",
              "CeV5pf/ZVgs6AE6jHnP9V0vjy07QBSMkcAfKezBGH4eiywthkFCoMvP0SkhveJ/sOFXVqKJroLsZ\n",
              "Lncx2m2VTMb2PTSXyJzbvU8gQ4kVUJcYsMsMr67Iej1i/M58r1ti0Xbw30D4bVEyvmY6HVhbyEom\n",
              "o//erLkUy+I/GfYU+gphmZMeaJPMFD4hbraG+IdcrJ1bO3XZSNjwg2r5BOp9kBBd4V6Jb1r0VuR5\n",
              "VsnDnau+aLSnHkHMs3QNT+Nq577UNAxPvUMlkZ5fgAAAADBBmiRsRn/FCW3eEPypLkK3QFEfwC6N\n",
              "pQ+1HEoLtxGmRUZl8+8sP0XR0wXVZRoUtkEAAAAmQZ5CeIS//LBYha+rFmQ/vF/SNFB1Y8SdXSqB\n",
              "TLvct/vjxl0GWNgAAAAbAZ5hdELf7+Ze4EuV93/+NjmpmZkCnX+tOCDBAAAADQGeY2pC3/vBfm2T\n",
              "r4EAAAAaQZpoSahBaJlMCO/gwEOpHPdxiwoL/ZT1hhkAAAAXQZ6GRREsIf/sx+GdcrXBoT1INPGP\n",
              "Y4EAAAAXAZ6ldELf7/D4SGBSv/xsc1MzMgU693wAAAAXAZ6nakLfwP+6dxQWjHzVfvVtMV9/PMAA\n",
              "AAAuQZqqSahBbJlMFExH1a3DWckrkWZJrb9EV8RASRur/X5eu1X/+vAToWH9Jev0wQAAABsBnslq\n",
              "Qt/4iQDa+1d+DANZQP/QsnukrWXboUAAAAAsQZrMSeEKUmUwUsV/Q+U4ACb53whQPtAMrYVKn+SG\n",
              "bIHkmLTLe2r2Ve2m+eYAAAAfAZ7rakLftpLJTUvBvDwgUac5js4Hr7BVT4ajMZtz+wAAABpBmu5J\n",
              "4Q6JlMFExn8P3DY26goIAx2hUHw14QAAABEBnw1qQt+IL7aT8yNtfXePIQAAABhBmw9J4Q8mUwIz\n",
              "/wqepB5TEMFFgx1vzlgAAAAZQZswSeEPJlMCM/8KCOMvjCcLd1COEGETgAAAADRBm1FJ4Q8mUwI7\n",
              "/1gJm1Sup3MLCH3nVbGX/JC0zWvRCAXzEdRJTGNXK7zs0Hi82p/ctsOBAAAAJkGbcknhDyZTAhD/\n",
              "jYyUWE9QFDyF4CBjs1mmRs5QnfgjF9JA/NLAAAAAHUGbk0nhDyZTAhL/cPvRoih9q8zO4rdc/2Gh\n",
              "zv5AAAAA0UGI7QCHxZj3u/9apz2IEScCLKaHECkG1+7oBwZe0gb3qsehlQbSQsesKCDy3fdZqsFN\n",
              "UYAYIhBDX5yXphalDQYAevW+cNaAVrXNafzfaH4L6UGQATb9HQS4EdcLDxbkrn/lokVc/TCLfl81\n",
              "yH0LjTu9sYI7HvuHD//PyXixCBcPPrzCv/4ImeiY9dwZpSvVhT+POv/F0OCoST3WFuhltPzHpnL/\n",
              "cIzimM4iSSLLVyFX+EWHPeXJ0Q4VF9Yy8zvx9dw2MpUu1lQ7/yU7LaHTbMEdAAAAHEGb2EnhDyZT\n",
              "Av/ZHAKJ/TjX2WKnhR9PsKIgXeAAAAAOQZ/2RRE83+yUwUuRu4EAAAAKAZ4VdELf7+wT+QAAAAsB\n",
              "nhdqQt/AdUu4OAAAACZBmhxJqEFomUwJ//iUVgU3EJMMkZN/ty/oWLovMcQPUzRuNJWCHQAAAA9B\n",
              "njpFESwl/8cliBhQY9gAAAAWAZ5ZdELfyjAe5is1NrpBgIT92cRhjQAAABcBnltqQt/OTiUKTfNv\n",
              "6L2jn9u7qlqggQAAABVBml5JqEFsmUwUTP/6aO5fSxIXjwYAAAALAZ59akLfw4qvZOAAAABAQZpg\n",
              "SeEKUmUwUsX/8f65DkGUlYElibnSHzzJ7NolVpQNPs5OICEYsNswliboqsRzV2oCbyD9F89lPX5E\n",
              "ECoX0wAAACYBnp9qQt/MfH73ihbgSrPlm/W4jGwvL+BeI2Hj3Aqd0NMbC+nb4AAAACNBmoFJ4Q6J\n",
              "lMCL//vq0BAc2Tti+GCfMOiiSYYVB12BcVnOmQAAAMlliIQAJ/++ngezvYhs+1j8QXIcOT1axCvr\n",
              "f86otyIRvf5FzbZpN3FL3q7XcZeuBI0Npo5D9FP4FAkmdNLRelx0Rw32huqSHJ7f8CLSXHJeSBx8\n",
              "s97lOB7zAmoa3fdlRyMZo0sWbGx+9XI0Zp9AL25UQZnuABZw+VbfMu/ibloceOpP0qHAb3emUBGl\n",
              "972QJ+/jEK44Le2t+xgRDSIA+NbArBs+QYObjOTIAMti21fFP/omRl5bHsNoRuqVKVbxJpJptL+4\n",
              "zONXH4AAAAA7QZokbEI/+XogVyAlwLW6mGNvfe3VTgdwE33OL1u/fMQ5p+/13ApjuYUkfxTvRJjT\n",
              "YIoUyTeN51NkX2cAAAAjQZ5CeIS/yWM/Nhkl1PXgHuQMXoeNQQdEXBZFre1/UpdHGe0AAAAdAZ5h\n",
              "dELfzb7eTwVvqRfA7oA/9q0/WutYMoLPm1kAAAASAZ5jakLfzH2kh2cdRmvfQ6HRAAAAMUGaaEmo\n",
              "QWiZTAm/qdQAQJeCC73tvYkldr1eytNBuv9IPCwELKkBUIXnE1y1wcCuJvoAAAAUQZ6GRREsKf/w\n",
              "OnGtsZxfMLnnBwQAAAAUAZ6ldELf+NbRCVXxhA+AkNu0eMEAAAALAZ6nakLfyZIKm+YAAAAhQZqs\n",
              "SahBbJlMCb846ZoIi7m4kCBKbUu2VFrWL5ZYMPoKAAAAI0GeykUVLCX/vAE1B+OzGRuuDEMzDTlw\n",
              "I9g3/DxOmVywiFCVAAAAHgGe6XRC38KNIv8bIk14qROaV2+3WSEhD71B2N3COQAAAAsBnutqQt9/\n",
              "iW8M4QAAAA5Bmu5JqEFsmUwUTEcCxgAAACABnw1qQt983P+AaaR6Vwu6Be0eWwFjI/xrxWoCHi8S\n",
              "lQAAACVBmxBJ4QpSZTBSxH8JRs7qOqD529CyEw6gExmqcL6xT7B1US3hAAAAIAGfL2pC34g7QwGp\n",
              "8H/fGnLGaSWStCdfqms2ebSv4l74AAAAJUGbMUnhDomUwIj/CD8O22s287CfeXGc77EdMUAoxgUk\n",
              "JSu9v/AAAAAhQZtSSeEPJlMCK/8MmMISudQRWBK2pKu6wvL0V+DaJrzhAAAAHkGbc0nhDyZTAiv/\n",
              "CLDB6o+GU2lkS9C2rRmXbQ8UwAAAACZBm5RJ4Q8mUwI7/xodL3HXC2DuCSIDgNAaMuF44M3MMEpL\n",
              "nzukUwAAACFBm7VJ4Q8mUwIr/wmUbd6OMsO5MgrHyGtsH872vogzScEAAACzQYj1gZ+7Sp0Vf7jt\n",
              "vsSC71EO+zcL5ZKggtYvnoBPcvzIILk64b1tum/V0g5PoKd7DaNKmOR4p0Ku31JdtjlBqzJlNLfa\n",
              "VxbrQGM7S0mK+oXwLmT8Q9LsBKCLYtwir7P/lUDWutclvBw2xODOGOH46P0fB0U8P5KKmDww0fcM\n",
              "HsocVRUmjNr/8smZ9q5XXtOdWd7sJIRs9P5Hc5kRbXS0FLZVDXSkWuZQ6c6JiUnsbOkad/gAAAAY\n",
              "QZv3SeEPJlMCFv8f2INDMwkYIDsoW+hXAAAAuUGIhg/uYCNVH8Ca/KJhYKesPwsMvTBRLjb43oTJ\n",
              "zBkaaIF1q0/wmukvktTQQJh03dJXus3N2ZaNc1fiun1ygQBYxcZH+3BjzscPZDXftH61Njf7Ps9m\n",
              "xR+rO25EWVpHneQhpZth2YP1LNWHnJSIZGP02xW/EQA958HRDTe5TVjwKPyu2vAiTsYmt6RpyPY7\n",
              "naQFYXPp0VhL9y80w17Lm6mBQ71AkKRskXpTpqKI2fho1UcvGTo0jaAeAAAAMkGaPEnhDyZTAm8Q\n",
              "5sAqg7YkZOfVcTu1bfDcmPJFKYZyuvondAmsVWAXpVDD2GfxMyOBAAAADkGeWkURPCX/kji01zGB\n",
              "AAAAHQGeeXRC351cFBu8GgGoKNaa4Ag+uNPn/id+jbFCAAAAEQGee2pC351bgBnnfDNQwdvKAAAA\n",
              "FkGaf0moQWiZTAiPEYZCLK3heK1nDEEAAAAcQZ6dRREsLf+f2U0vRQG0J5BeraDVWqRV4IIc4AAA\n",
              "AA4Bnr5qQt+dK97nnq2G4QAAAEBBmqBJqEFsmUwIjz4cdjM89PBgws9dJIfx10vLpWZqSOJ2ANc8\n",
              "CJI3miVuVuIef/zpRbf756e7NWE/OxgTL4HAAAAAK0GawUnhClJlMCI/PvW7GZ56eDI1e69Lu9we\n",
              "+1ZQ//juBcXctogac+kwZoEAAAAqQZriSeEOiZTAiv8ggO/uD80Po/P3PZKfuCCyivyNZ1v1kiye\n",
              "xDvV15lVAAAAJEGbA0nhDyZTAiv/M3sbRBl529yzfjCbB2ZdoQeDv9+uNsmxiQAAADpBmyZJ4Q8m\n",
              "UwI7/8abFutZf+nL25g/kE54xGoMIvWHj6gblEkopO6UR/TvqUrVGN/U274t6uOZE03BAAAAJEGf\n",
              "REURPC3/8UVJcUqG/I3aWdb7X4jIodUGc8C/E7LQOp5UgAAAADUBn2VqQt/w9zX3ZklhBoRYMMl6\n",
              "wmmt/5i7GvXJQW4kLzxKdueQ37Bolsq+5gKh7/325rrKjAAAACxBm2hJqEFomUwU8Jd6EjnvmD/w\n",
              "AwsJCBFm2FtyZ5vY1phmiDopbVnzx82IgQAAACABn4dqQt/OT+Exb/hX21KKyYY1bdGY/qXwFVsz\n",
              "WpP+SAAAANZliIIAFf/YKGLv/ufyPOpr8spD3BwY+XGD9jiHW9dABsJp2beJwMXa4ntdPdhuV3LT\n",
              "i8XWKjIsfxfF9lcmfCueC0MpDgmyRp5azkxbbwCBI2tmMfBFJ2rF6wY6QzvR7Diy3ngvSlzC+H4/\n",
              "7V3GSqpVb//6sZreObLl6MAoumZapdtgt7jFeRKJh8sN3NilyHDmnKwJsQCjl+EtTAcODDwxf2Ce\n",
              "g3iOXWapTA7mMNvZT49uiqj8JfaJ0D0LmfDMuSjCTYF6HITzdb+H1wttELvW1mVeWLGAAAAAHUGa\n",
              "JGxCX9FqYNFIWEV9cxJirKcyJILJj6CKen2ZAAAAGUGeQniFP/04IN5XyTPZCMU7WSw45e0zgOEA\n",
              "AAAQAZ5hdELf9SiMbXr3I8f7rQAAAAwBnmNqQt/w3ralEZAAAAARQZplSahBaJlMCFv/IAFI1PAA\n",
              "AAfJbW9vdgAAAGxtdmhkAAAAAAAAAAAAAAAAAAAD6AAADtgAAQAAAQAAAAAAAAAAAAAAAAEAAAAA\n",
              "AAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
              "AgAABvN0cmFrAAAAXHRraGQAAAADAAAAAAAAAAAAAAABAAAAAAAADtgAAAAAAAAAAAAAAAAAAAAA\n",
              "AAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAEAAAABAAAAAAAAkZWR0cwAAABxl\n",
              "bHN0AAAAAAAAAAEAAA7YAAAEAAABAAAAAAZrbWRpYQAAACBtZGhkAAAAAAAAAAAAAAAAAAA8AAAA\n",
              "5ABVxAAAAAAALWhkbHIAAAAAAAAAAHZpZGUAAAAAAAAAAAAAAABWaWRlb0hhbmRsZXIAAAAGFm1p\n",
              "bmYAAAAUdm1oZAAAAAEAAAAAAAAAAAAAACRkaW5mAAAAHGRyZWYAAAAAAAAAAQAAAAx1cmwgAAAA\n",
              "AQAABdZzdGJsAAAAsnN0c2QAAAAAAAAAAQAAAKJhdmMxAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAA\n",
              "AEAAQABIAAAASAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGP//AAAA\n",
              "MGF2Y0MBZAAK/+EAF2dkAAqs2UQmhAAAAwAEAAADAPA8SJZYAQAGaOvjyyLAAAAAHHV1aWRraEDy\n",
              "XyRPxbo5pRvPAyPzAAAAAAAAABhzdHRzAAAAAAAAAAEAAAByAAACAAAAACBzdHNzAAAAAAAAAAQA\n",
              "AAABAAAAIgAAAEQAAABtAAAC2GN0dHMAAAAAAAAAWQAAAAEAAAQAAAAAAQAABgAAAAABAAACAAAA\n",
              "AAQAAAQAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAACAAAEAAAAAAEAAAoAAAAA\n",
              "AQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAAAIAAAQAAAAAAQAACgAAAAAB\n",
              "AAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEA\n",
              "AAYAAAAAAQAAAgAAAAACAAAEAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAA\n",
              "CgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAABgAAAAABAAAC\n",
              "AAAAAAEAAAYAAAAAAQAAAgAAAAAGAAAEAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIA\n",
              "AAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAABgAA\n",
              "AAABAAACAAAAAAIAAAQAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAA\n",
              "AAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAA\n",
              "AQAABgAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAAIAAAEAAAAAAEAAAoAAAAAAQAABAAAAAAB\n",
              "AAAAAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAAAQAAAQAAAAAAQAACAAAAAACAAACAAAAAAEA\n",
              "AAYAAAAAAQAAAgAAAAABAAAEAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAA\n",
              "BAAAAAAcc3RzYwAAAAAAAAABAAAAAQAAAHIAAAABAAAB3HN0c3oAAAAAAAAAAAAAAHIAAAOVAAAA\n",
              "LwAAACkAAAAVAAAAGAAAACkAAADaAAAAPQAAACUAAAAVAAAAKwAAANgAAAATAAAAKQAAACUAAAAY\n",
              "AAAAGwAAACkAAAAVAAAAEQAAACgAAAC7AAAAOAAAACIAAAAqAAAAEgAAABoAAAAPAAAADQAAAAwA\n",
              "AAAcAAAAFwAAAC4AAADUAAAANAAAACoAAAAfAAAAEQAAAB4AAAAbAAAAGwAAABsAAAAyAAAAHwAA\n",
              "ADAAAAAjAAAAHgAAABUAAAAcAAAAHQAAADgAAAAqAAAAIQAAANUAAAAgAAAAEgAAAA4AAAAPAAAA\n",
              "KgAAABMAAAAaAAAAGwAAABkAAAAPAAAARAAAACoAAAAnAAAAzQAAAD8AAAAnAAAAIQAAABYAAAA1\n",
              "AAAAGAAAABgAAAAPAAAAJQAAACcAAAAiAAAADwAAABIAAAAkAAAAKQAAACQAAAApAAAAJQAAACIA\n",
              "AAAqAAAAJQAAALcAAAAcAAAAvQAAADYAAAASAAAAIQAAABUAAAAaAAAAIAAAABIAAABEAAAALwAA\n",
              "AC4AAAAoAAAAPgAAACgAAAA5AAAAMAAAACQAAADaAAAAIQAAAB0AAAAUAAAAEAAAABUAAAAUc3Rj\n",
              "bwAAAAAAAAABAAAALAAAAGJ1ZHRhAAAAWm1ldGEAAAAAAAAAIWhkbHIAAAAAAAAAAG1kaXJhcHBs\n",
              "AAAAAAAAAAAAAAAALWlsc3QAAAAlqXRvbwAAAB1kYXRhAAAAAQAAAABMYXZmNTcuODMuMTAw\n",
              "\">\n",
              "  Your browser does not support the video tag.\n",
              "</video>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "    \n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "        \n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)"
      ],
      "metadata": {
        "id": "hJmJ6-qdarH4"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uyh1BJqqtsE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RLDataset(IterableDataset):\n",
        "    def __init__(self, buffer, sample_size=400):\n",
        "        self.buffer = buffer\n",
        "        self.sample_size = sample_size\n",
        "    \n",
        "    def __iter__(self):\n",
        "        for experience in self.buffer.sample(self.sample_size):\n",
        "            yield experience"
      ],
      "metadata": {
        "id": "ttUGSxKlbh4Y"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(state, env, net, epsilon=0.0):\n",
        "    if np.random.random() < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        state = state.to(device)\n",
        "        q_values = net(state)\n",
        "        _, action = torch.max(q_values, dim=1)\n",
        "        action = int(action.item())\n",
        "    return action"
      ],
      "metadata": {
        "id": "PVR51pCNnEQv"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size , n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        \n",
        "        self.state_size = state_size\n",
        "        self.conv = nn.Sequential(\n",
        "                        nn.Conv2d(state_size[0], 32, 3, stride=2, padding=1),\n",
        "                        nn.ELU(),\n",
        "                        nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
        "                        nn.ELU(),\n",
        "                        nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
        "                        nn.ELU(),\n",
        "                        nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
        "                        nn.ELU()\n",
        "                    )\n",
        "        conv_out_size = self._get_conv_out(state_size)\n",
        "        self.fc1 = nn.Linear(conv_out_size, 256)\n",
        "        self.fc_adv = nn.Linear(256, n_actions) \n",
        "        self.fc_value = nn.Linear(256, 1)\n",
        "        \n",
        "    def _get_conv_out(self, shape):\n",
        "        conv_out = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(conv_out.size()))\n",
        "    \n",
        "    def forward(self, x):        \n",
        "        o = self.conv(x.float()).view(x.shape[0], -1)\n",
        "        o = F.relu(self.fc1(o))\n",
        "        \n",
        "        adv = self.fc_adv(o)\n",
        "        value = self.fc_value(o)  \n",
        "        \n",
        "        return value + adv - torch.mean(adv, dim=1, keepdim=True)"
      ],
      "metadata": {
        "id": "VLKxI_OynWeO"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepQLearning(LightningModule):\n",
        "    def __init__(self, env_name, policy=epsilon_greedy, capacity=100_000, \n",
        "               batch_size=256, lr=1e-3, hidden_size=128, gamma=0.99, \n",
        "               loss_fn=nn.MSELoss(), optim=AdamW, eps_start=1.0, eps_end=0.15, \n",
        "               eps_last_episode=400, samples_per_epoch=1024, sync_rate=10,\n",
        "               sequence_length = 8):\n",
        "    \n",
        "        super().__init__()\n",
        "        self.env = create_env(env_name, 2)\n",
        "\n",
        "        obs_size = (1, 64, 64)\n",
        "        n_actions = 6\n",
        "\n",
        "        self.q_net = DQN(obs_size, n_actions)\n",
        "\n",
        "        self.target_q_net = copy.deepcopy(self.q_net)\n",
        "\n",
        "        self.policy = policy\n",
        "        self.buffer = ReplayBuffer(capacity=capacity)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "            print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "            self.play_episode(epsilon=self.hparams.eps_start)\n",
        "            \n",
        "    @torch.no_grad()\n",
        "    def play_episode(self, policy=None, epsilon=0.):\n",
        "        state  = self.env.reset()\n",
        "        state  = torch.from_numpy(state[0]).unsqueeze(dim=0)\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            if policy:\n",
        "                action = policy(state.unsqueeze(dim=0), self.env, self.q_net, epsilon=epsilon)\n",
        "            else:\n",
        "                action = self.env.action_space.sample()\n",
        "            next_state, reward, done, tru , _ = self.env.step(action)\n",
        "            if tru:\n",
        "                done = tru\n",
        "            \n",
        "            next_state = torch.from_numpy(next_state).unsqueeze(dim=0) \n",
        "            exp = (state, action, reward, done, next_state)\n",
        "            \n",
        "            self.buffer.append(exp)\n",
        "            state = next_state\n",
        "            \n",
        "        self.env.close()\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.q_net(x)\n",
        "\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n",
        "        return [q_net_optimizer]\n",
        "\n",
        "     # Create dataloader.\n",
        "    def train_dataloader(self):\n",
        "        dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "       \n",
        "        dataloader = DataLoader(\n",
        "            dataset=dataset,\n",
        "            batch_size=self.hparams.batch_size\n",
        "        )\n",
        "        return dataloader\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        states, actions, rewards, dones, next_states = batch\n",
        "        actions = actions.unsqueeze(1)\n",
        "        rewards = rewards.unsqueeze(1)\n",
        "        dones = dones.unsqueeze(1)\n",
        "        \n",
        "        state_action_values = self.q_net(states).gather(1, actions)\n",
        "\n",
        "        next_action_values, _ = self.target_q_net(next_states).max(dim=1, keepdim=True)\n",
        "        next_action_values[dones] = 0.0\n",
        "\n",
        "        expected_state_action_values = rewards + self.hparams.gamma * next_action_values\n",
        "\n",
        "        loss = self.hparams.loss_fn(state_action_values.float(), expected_state_action_values.float())\n",
        "        self.log('episode/Q-Error', loss)\n",
        "        return loss\n",
        "    \n",
        "    # Training epoch end.\n",
        "    def training_epoch_end(self, training_step_outputs):\n",
        "        epsilon = max(\n",
        "            self.hparams.eps_end,\n",
        "            self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
        "        )\n",
        "\n",
        "        self.play_episode(policy=self.policy, epsilon=epsilon)\n",
        "        self.log('episode/Return', self.env.return_queue[-1])\n",
        "\n",
        "        if self.current_epoch % self.hparams.sync_rate == 0:\n",
        "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
        "            \n",
        "            \n",
        "    def save_model(self):\n",
        "        torch.save(self.q_net.state_dict(), \"./model\")\n",
        "        \n",
        "    def load_model(self):\n",
        "        self.q_net.load_state_dict(torch.load( \"./model\"))\n"
      ],
      "metadata": {
        "id": "OflFw6LqdFNb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}