{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "618897fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\multi_agents_rl\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "from collections import deque\n",
    "import itertools\n",
    "\n",
    "import gym\n",
    "from gym.spaces.box import Box\n",
    "from gym import wrappers\n",
    "from gym.wrappers import TransformObservation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit, AtariPreprocessing\n",
    "\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "num_gpus = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45cb7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRQN(nn.Module):\n",
    "    def __init__(self, state_size , n_actions):\n",
    "        super(DRQN, self).__init__()\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.conv = nn.Sequential(\n",
    "                        nn.Conv2d(state_size[0], 32, 3, stride=2, padding=1),\n",
    "                        nn.ELU(),\n",
    "                        nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
    "                        nn.ELU(),\n",
    "                        nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
    "                        nn.ELU(),\n",
    "                        nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
    "                        nn.ELU()\n",
    "                    )\n",
    "        conv_out_size = self._get_conv_out(state_size)\n",
    "        self.fc1 = nn.Linear(conv_out_size, 256)\n",
    "        self.fc_adv = nn.Linear(256, n_actions) \n",
    "        self.fc_value = nn.Linear(256, 1)\n",
    "        \n",
    "    def _get_conv_out(self, shape):\n",
    "        conv_out = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(conv_out.size()))\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        o = self.conv(x.float()).view(x.shape[0], -1)\n",
    "        o = F.relu(self.fc1(o))\n",
    "        \n",
    "        adv = self.fc_adv(o)\n",
    "        value = self.fc_value(o)  \n",
    "        \n",
    "        return value + adv - torch.mean(adv, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d91fbd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, env, net, epsilon=0.0):\n",
    "    if np.random.random() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        state = state.to(device)\n",
    "        q_values = net(state)\n",
    "        _, action = torch.max(q_values, dim=1)\n",
    "        action = int(action.item())\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec9d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51c92245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    def __init__(self, buffer, sample_size=400):\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for experience in self.buffer.sample(self.sample_size):\n",
    "            yield experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bb78db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(name):\n",
    "    env = gym.make(name, render_mode=\"rgb_array\")\n",
    "    env.unwrapped._frameskip = 1\n",
    "    env = TimeLimit(env, max_episode_steps=400)\n",
    "    env = RecordVideo(env, video_folder='./videos/drqn-pong', episode_trigger=lambda x: x % 50 == 0)\n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    env = gym.wrappers.AtariPreprocessing(env, frame_skip=8, noop_max=28, screen_size=64, terminal_on_life_loss=False, grayscale_obs=True, grayscale_newaxis=False, scale_obs=True)\n",
    "    env.observation_space = Box(0.0, 1.0, [1, 64, 64])\n",
    "    return env\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1040f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQLearning(LightningModule):\n",
    "    def __init__(self, env_name, policy=epsilon_greedy, capacity=100_000, \n",
    "               batch_size=256, lr=1e-3, hidden_size=128, gamma=0.99, \n",
    "               loss_fn=nn.MSELoss(), optim=AdamW, eps_start=1.0, eps_end=0.15, \n",
    "               eps_last_episode=400, samples_per_epoch=1024, sync_rate=10,\n",
    "               sequence_length = 8):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.env = create_environment(env_name)\n",
    "\n",
    "        obs_size = self.env.observation_space.shape\n",
    "        n_actions = self.env.action_space.n\n",
    "\n",
    "        self.q_net = DRQN(obs_size, n_actions)\n",
    "\n",
    "        self.target_q_net = copy.deepcopy(self.q_net)\n",
    "\n",
    "        self.policy = policy\n",
    "        self.buffer = ReplayBuffer(capacity=capacity)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        while len(self.buffer) < self.hparams.samples_per_epoch:\n",
    "            print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
    "            self.play_episode(epsilon=self.hparams.eps_start)\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def play_episode(self, policy=None, epsilon=0.):\n",
    "        state  = self.env.reset()\n",
    "        state  = torch.from_numpy(state[0]).unsqueeze(dim=0)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if policy:\n",
    "                action = policy(state.unsqueeze(dim=0), self.env, self.q_net, epsilon=epsilon)\n",
    "            else:\n",
    "                action = self.env.action_space.sample()\n",
    "            next_state, reward, done, tru , _ = self.env.step(action)\n",
    "            if tru:\n",
    "                done = tru\n",
    "            \n",
    "            next_state = torch.from_numpy(next_state).unsqueeze(dim=0) \n",
    "            exp = (state, action, reward, done, next_state)\n",
    "            \n",
    "            self.buffer.append(exp)\n",
    "            state = next_state\n",
    "            \n",
    "        self.env.close()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.q_net(x)\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n",
    "        return [q_net_optimizer]\n",
    "\n",
    "     # Create dataloader.\n",
    "    def train_dataloader(self):\n",
    "        dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
    "       \n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size\n",
    "        )\n",
    "        return dataloader\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        actions = actions.unsqueeze(1)\n",
    "        rewards = rewards.unsqueeze(1)\n",
    "        dones = dones.unsqueeze(1)\n",
    "        \n",
    "        state_action_values = self.q_net(states).gather(1, actions)\n",
    "\n",
    "        next_action_values, _ = self.target_q_net(next_states).max(dim=1, keepdim=True)\n",
    "        next_action_values[dones] = 0.0\n",
    "\n",
    "        expected_state_action_values = rewards + self.hparams.gamma * next_action_values\n",
    "\n",
    "        loss = self.hparams.loss_fn(state_action_values.float(), expected_state_action_values.float())\n",
    "        self.log('episode/Q-Error', loss)\n",
    "        return loss\n",
    "    \n",
    "    # Training epoch end.\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        epsilon = max(\n",
    "            self.hparams.eps_end,\n",
    "            self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
    "        )\n",
    "\n",
    "        self.play_episode(policy=self.policy, epsilon=epsilon)\n",
    "        self.log('episode/Return', self.env.return_queue[-1])\n",
    "\n",
    "        if self.current_epoch % self.hparams.sync_rate == 0:\n",
    "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "            \n",
    "            \n",
    "    def save_model(self):\n",
    "        torch.save(self.q_net.state_dict(), \"./model\")\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.q_net.load_state_dict(torch.load( \"./model\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a5d649b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\multi_agents_rl\\env\\lib\\site-packages\\gym\\wrappers\\record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at D:\\multi_agents_rl\\videos\\drqn-pong folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "D:\\multi_agents_rl\\env\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:268: UserWarning: Attribute 'loss_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_fn'])`.\n",
      "  rank_zero_warn(\n",
      "D:\\multi_agents_rl\\env\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py:59: UserWarning: \u001b[33mWARN: Disabling video recorder because environment <TimeLimit<OrderEnforcing<PassiveEnvChecker<AtariEnv<ALE/Pong-v5>>>>> was not initialized with any compatible video mode between `rgb_array` and `rgb_array_list`\u001b[0m\n",
      "  logger.warn(\n",
      "D:\\multi_agents_rl\\env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:289: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 samples in experience buffer. Filling...\n",
      "48 samples in experience buffer. Filling...\n",
      "95 samples in experience buffer. Filling...\n",
      "144 samples in experience buffer. Filling...\n",
      "191 samples in experience buffer. Filling...\n",
      "241 samples in experience buffer. Filling...\n",
      "288 samples in experience buffer. Filling...\n",
      "336 samples in experience buffer. Filling...\n",
      "383 samples in experience buffer. Filling...\n",
      "431 samples in experience buffer. Filling...\n",
      "480 samples in experience buffer. Filling...\n",
      "527 samples in experience buffer. Filling...\n",
      "576 samples in experience buffer. Filling...\n",
      "625 samples in experience buffer. Filling...\n",
      "675 samples in experience buffer. Filling...\n",
      "723 samples in experience buffer. Filling...\n",
      "771 samples in experience buffer. Filling...\n",
      "820 samples in experience buffer. Filling...\n",
      "869 samples in experience buffer. Filling...\n",
      "917 samples in experience buffer. Filling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965 samples in experience buffer. Filling...\n",
      "1014 samples in experience buffer. Filling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\multi_agents_rl\\env\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:616: UserWarning: Checkpoint directory D:\\multi_agents_rl\\checkpoints\\drqb-pong exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type | Params\n",
      "--------------------------------------\n",
      "0 | q_net        | DRQN | 161 K \n",
      "1 | target_q_net | DRQN | 161 K \n",
      "--------------------------------------\n",
      "322 K     Trainable params\n",
      "0         Non-trainable params\n",
      "322 K     Total params\n",
      "1.290     Total estimated model params size (MB)\n",
      "D:\\multi_agents_rl\\env\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5659: : 1it [00:00, 23.95it/s, loss=0.0129, v_num=5] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\multi_agents_rl\\env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "algo = DeepQLearning('ALE/Pong-v5')\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"./checkpoints/drqb-pong\", save_top_k=1,mode=\"max\", monitor=\"episode/Return\")\n",
    "\n",
    "trainer = Trainer(\n",
    "     accelerator='gpu',\n",
    "     devices=num_gpus,\n",
    "     max_epochs=20_000,\n",
    "     callbacks=[checkpoint_callback], # EarlyStopping(monitor='episode/Return', mode='max', patience=1000)\n",
    ")\n",
    "\n",
    "trainer.fit(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b220609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "frames = []\n",
    "env = create_environment(\"ALE/Pong-v5\")\n",
    "for episode in range(1):\n",
    "  done = False\n",
    "  obs, _  = env.reset()\n",
    "  while not done:\n",
    "    frames.append(obs)\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, t ,_ = env.step(action)\n",
    "    #print(reward)\n",
    "    if t:\n",
    "        done = True\n",
    "    env.close()\n",
    "    \n",
    "print(len(frames))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6666ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = algo.env\n",
    "policy = algo.policy\n",
    "q_net = algo.q_net.cuda()\n",
    "frames = []\n",
    "\n",
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    state = torch.from_numpy(state).unsqueeze(dim=0)\n",
    "    done = False\n",
    "    hidden = None\n",
    "    while not done:\n",
    "        action, hidden = policy(state, env, q_net, epsilon=0, hidden=hidden)\n",
    "        next_state, reward, done, tru , _ = env.step(action)\n",
    "        next_state = torch.from_numpy(next_state).unsqueeze(dim=0)\n",
    "        if tru:\n",
    "            done = tru\n",
    "        state = next_state\n",
    "        frame = state.squeeze(dim=0)\n",
    "        frame = frame.numpy()\n",
    "        frames.append(frame)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "10bfb4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23529412 0.23529412 0.23529412 ... 0.34117648 0.34117648 0.34117648]\n",
      " [0.34117648 0.34117648 0.34117648 ... 0.34117648 0.34117648 0.34117648]\n",
      " [0.34117648 0.34117648 0.34117648 ... 0.34117648 0.34117648 0.34117648]\n",
      " ...\n",
      " [0.9254902  0.9254902  0.9254902  ... 0.9254902  0.9254902  0.9254902 ]\n",
      " [0.9254902  0.9254902  0.9254902  ... 0.9254902  0.9254902  0.9254902 ]\n",
      " [0.9254902  0.9254902  0.9254902  ... 0.9254902  0.9254902  0.9254902 ]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "print(frames[0].shape)\n",
    "for frame in frames:\n",
    "    plt.imshow(frame)\n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n",
    "print(frames[0])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = algo.train_dataloader()\n",
    "\n",
    "ite = iter(loader)\n",
    "\n",
    "x = ite.next()\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e18bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.tensor([[1,2,3,4],[1,2,3,4]]))\n",
    "x = x.squeeze(dim=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d1bc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[[1,2,3,4]]])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[[[[1,1]]],[[[2,2]]],[[[3,3]]],[[[4,4]]],[[[5,5]]],[[[6,6]]]],[[[[7,7]]],[[[8,8]]],[[[1,2]]],[[[1,2]]],[[[1,2]]],[[[1,2]]]]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d66220",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,3,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08022a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,1])\n",
    "y = torch.tensor([2,2])\n",
    "stack = torch.stack([x,y],dim=0)\n",
    "stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b0c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = torch.tensor([3,3]).unsqueeze(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f019ef81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a87c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,1,1,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c215618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1,2,3,4,5,6]\n",
    "\n",
    "for i in l[::2]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619979e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,1,1,1,1],[2,2,2,2,2],[3,3,3,3,3],[4,4,4,4,4]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c072540",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        print(x[i+j:2:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f4a607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ReplayBuffer(10)\n",
    "x.append((1,1,0))\n",
    "x.append((1,1,0))\n",
    "x.append((1,1,1))\n",
    "x.append((1,1,0))\n",
    "x.append((1,1,0))\n",
    "x.append((1,1,-1))\n",
    "x.append((1,1,0))\n",
    "x.append((1,1,0))\n",
    "x.append((1,1,0))\n",
    "x.append((1,1,1))\n",
    "x.append((1,1,0))\n",
    "x.append((1,1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d954e67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 0),\n",
       " (1, 1, 0),\n",
       " (1, 1, -1),\n",
       " (1, 1, 0),\n",
       " (1, 1, 0),\n",
       " (1, 1, -1),\n",
       " (1, 1, 0),\n",
       " (1, 1, 0),\n",
       " (1, 1, -1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sample(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "04a70af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 0),\n",
       " (1, 1, 0),\n",
       " (1, 1, 1),\n",
       " (1, 1, 0),\n",
       " (1, 1, 0),\n",
       " (1, 1, -1),\n",
       " (1, 1, 0),\n",
       " (1, 1, 0),\n",
       " (1, 1, 1),\n",
       " (1, 1, 1),\n",
       " (1, 1, 0),\n",
       " (1, 1, -1)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa3145d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
