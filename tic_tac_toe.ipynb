{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "122b6a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\multi_agents_rl\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "from collections import deque\n",
    "import itertools\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from pyvirtualdisplay import Display\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "num_gpus = torch.cuda.device_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95544e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_actions),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x.float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "57d479f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, env, net,mask ,epsilon=0.0):\n",
    "    if np.random.random() < epsilon:\n",
    "        action = random.choice(np.argwhere(mask==1).reshape(-1))\n",
    "    else:\n",
    "        state = state.to(device)\n",
    "        q_values = net(state) * torch.from_numpy(mask).to(device)\n",
    "        q_values[q_values == 0] =  -1\n",
    "        _, action = torch.max(q_values, dim=1) \n",
    "        action = int(action.item())\n",
    "    return action\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "398c3d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f7fac7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    def __init__(self, buffer, sample_size=400):\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for experience in self.buffer.sample(self.sample_size):\n",
    "            yield experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8014297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQLearning(LightningModule):\n",
    "    def __init__(self, policy=epsilon_greedy, capacity=50_000, \n",
    "               batch_size=512, lr=0.001, hidden_size=128, gamma=0.99, \n",
    "               loss_fn=F.smooth_l1_loss, optim=AdamW, eps_start=1.0, eps_end=0.2, \n",
    "               eps_last_episode=400, samples_per_epoch=1024, sync_rate=10,\n",
    "               sequence_length = 8):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.env = tictactoe_v3.env()\n",
    "\n",
    "        obs_size = 18\n",
    "        n_actions = 9\n",
    "\n",
    "        \n",
    "        self.q_net = DQN(obs_size, n_actions)\n",
    "        self.target_q_net = copy.deepcopy(self.q_net)\n",
    "\n",
    "        self.q_net1 = DQN(obs_size, n_actions)\n",
    "        self.target_q_net1 = copy.deepcopy(self.q_net1)\n",
    "\n",
    "        self.policy = policy\n",
    "        self.buffer = ReplayBuffer(capacity=capacity)\n",
    "        self.buffer1 = ReplayBuffer(capacity=capacity)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        while len(self.buffer) < self.hparams.samples_per_epoch:\n",
    "            self.play_episode(epsilon=self.hparams.eps_start)\n",
    "        print(f\"buffer 1  {len(self.buffer)} samples in experience buffer. Filling...\")\n",
    "\n",
    "        while len(self.buffer1) < self.hparams.samples_per_epoch:\n",
    "            self.play_episode(epsilon=self.hparams.eps_start)\n",
    "            \n",
    "        print(f\"buffer 2: {len(self.buffer1)} samples in experience buffer. Filling...\")\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def play_episode(self, policy=None, epsilon=0.):\n",
    "        self.env.reset()\n",
    "        prev_state = {'player_1': torch.zeros((18)),\n",
    "                      'player_2': torch.zeros((18))}\n",
    "        prev_action = {'player_1': 0, 'player_2': 0}\n",
    "        returns = {'player_1': 0, 'player_2': 0}\n",
    "        for agent in self.env.agent_iter():\n",
    "          observation_mask, reward, done, info = self.env.last()\n",
    "          returns[agent] += reward\n",
    "          observation = torch.from_numpy(observation_mask[\"observation\"].reshape(-1)) \n",
    "          exp = (prev_state[agent], prev_action[agent], reward, done, observation)\n",
    "          if agent == \"player_1\":\n",
    "            self.buffer.append(exp)\n",
    "          else:\n",
    "            self.buffer1.append(exp)\n",
    "          prev_state[agent] = observation\n",
    "          if agent == \"player_1\":            \n",
    "            if not done:\n",
    "                  if policy: \n",
    "                    action = policy(observation.unsqueeze(dim=0), self.env, self.q_net,observation_mask[\"action_mask\"], epsilon=epsilon)\n",
    "                  else:\n",
    "                    action = random.choice(np.argwhere(observation_mask[\"action_mask\"] ==1).reshape(-1))\n",
    "                  prev_action[agent] = action\n",
    "                  self.env.step(action)\n",
    "            else: \n",
    "                  self.env.step(None)\n",
    "          else:\n",
    "            \n",
    "            if not done:\n",
    "                  if policy: \n",
    "                    action = policy(observation.unsqueeze(dim=0), self.env, self.q_net1,observation_mask[\"action_mask\"], epsilon=epsilon)\n",
    "                  else:\n",
    "                    action = random.choice(np.argwhere(observation_mask[\"action_mask\"] ==1).reshape(-1))\n",
    "                  prev_action[agent] = action\n",
    "                  self.env.step(action)\n",
    "            else: \n",
    "                  self.env.step(None)\n",
    "        \n",
    "        if policy:\n",
    "          self.log(\"episode/Return/agent_1\",returns['player_1'])\n",
    "          self.log(\"episode/Return/agent_2\",returns['player_2'])\n",
    "       \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def play_episode_test(self, epsilon=0.):\n",
    "        returns1 = 0\n",
    "        for i in range(100):\n",
    "            self.env.reset()\n",
    "            for agent in self.env.agent_iter():\n",
    "                observation_mask, reward, done, info = self.env.last()\n",
    "                observation = torch.from_numpy(observation_mask[\"observation\"].reshape(-1)).unsqueeze(dim=0)\n",
    "                if agent == \"player_1\":\n",
    "                    returns1 += reward\n",
    "                    if not done:\n",
    "                        action = self.policy(observation, self.env, self.q_net, observation_mask[\"action_mask\"], epsilon=0)\n",
    "                    else:\n",
    "                        action = None\n",
    "                    self.env.step(action)\n",
    "                else:\n",
    "                    if not done:\n",
    "                        action = random.choice(np.argwhere(observation_mask[\"action_mask\"] ==1).reshape(-1))\n",
    "                    else:\n",
    "                        action = None\n",
    "                    self.env.step(action)\n",
    "        \n",
    "        returns2 = 0\n",
    "        for i in range(100):\n",
    "            self.env.reset()\n",
    "            for agent in self.env.agent_iter():\n",
    "                observation_mask, reward, done, info = self.env.last()\n",
    "                observation = torch.from_numpy(observation_mask[\"observation\"].reshape(-1)).unsqueeze(dim=0)\n",
    "                if agent == \"player_1\":\n",
    "                    if not done:\n",
    "                        action = random.choice(np.argwhere(observation_mask[\"action_mask\"] ==1).reshape(-1))\n",
    "                    else:\n",
    "                        action = None\n",
    "                    self.env.step(action)\n",
    "                else:\n",
    "                    returns2 += reward\n",
    "\n",
    "                    if not done:\n",
    "                        action = self.policy(observation, self.env, self.q_net1, observation_mask[\"action_mask\"], epsilon=0)\n",
    "                    else:\n",
    "                        action = None\n",
    "                    self.env.step(action)\n",
    "        self.log(\"episode/ReturnValidation/agent_1\",returns1/100)\n",
    "        self.log(\"episode/ReturnValidation/agent_2\",returns2/100)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.q_net(x)\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n",
    "        q_net_optimizer1 = self.hparams.optim(self.q_net1.parameters(), lr=self.hparams.lr)\n",
    "        return [q_net_optimizer, q_net_optimizer1]\n",
    "\n",
    "     # Create dataloader.\n",
    "    def train_dataloader(self):\n",
    "\n",
    "        dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
    "        dataset1 = RLDataset(self.buffer1, self.hparams.samples_per_epoch)\n",
    "\n",
    "        loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size \n",
    "        )\n",
    "\n",
    "        loader1 = DataLoader(\n",
    "            dataset=dataset1,\n",
    "            batch_size=self.hparams.batch_size\n",
    "        )\n",
    "        return [loader, loader1]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        batch1 = batch[0]\n",
    "        batch2 = batch[1]\n",
    "       \n",
    "        if optimizer_idx == 0:\n",
    "            states, actions, rewards, dones, next_states = batch1\n",
    "            actions = actions.unsqueeze(1)\n",
    "            rewards = rewards.unsqueeze(1)\n",
    "            dones = dones.unsqueeze(1)\n",
    "\n",
    "            state_action_values = self.q_net(states).gather(1, actions)\n",
    "\n",
    "            next_action_values, _ = self.target_q_net(next_states).max(dim=1, keepdim=True)\n",
    "            next_action_values[dones] = 0.0\n",
    "\n",
    "            expected_state_action_values = rewards + self.hparams.gamma * next_action_values\n",
    "\n",
    "            loss = self.hparams.loss_fn(state_action_values.float(), expected_state_action_values.float())\n",
    "            self.log('episode/Q-Error1', loss)\n",
    "            return loss\n",
    "\n",
    "        if optimizer_idx == 1:\n",
    "            states, actions, rewards, dones, next_states = batch2\n",
    "            actions = actions.unsqueeze(1)\n",
    "            rewards = rewards.unsqueeze(1)\n",
    "            dones = dones.unsqueeze(1)\n",
    "\n",
    "            state_action_values = self.q_net1(states).gather(1, actions)\n",
    "\n",
    "            next_action_values, _ = self.target_q_net1(next_states).max(dim=1, keepdim=True)\n",
    "            next_action_values[dones] = 0.0\n",
    "\n",
    "            expected_state_action_values = rewards + self.hparams.gamma * next_action_values\n",
    "\n",
    "            loss = self.hparams.loss_fn(state_action_values.float(), expected_state_action_values.float())\n",
    "            self.log('episode/Q-Error2', loss)\n",
    "            return loss\n",
    "    \n",
    "    # Training epoch end.\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        epsilon = max(\n",
    "            self.hparams.eps_end,\n",
    "            self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
    "        )\n",
    "\n",
    "        self.play_episode(policy=self.policy, epsilon=epsilon)\n",
    "\n",
    "        if self.current_epoch % self.hparams.sync_rate == 0:\n",
    "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "            self.target_q_net1.load_state_dict(self.q_net1.state_dict())\n",
    "        \n",
    "        if self.current_epoch % 100 == 0:\n",
    "            self.play_episode_test()\n",
    "        clear_output(wait=True)\n",
    "            \n",
    "            \n",
    "    def save_model(self):\n",
    "        torch.save(self.q_net.state_dict(), \"./model\")\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.q_net.load_state_dict(torch.load( \"./model\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4534ef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1400: : 2it [00:00, 24.11it/s, loss=11.5, v_num=41]"
     ]
    }
   ],
   "source": [
    "algo = DeepQLearning()\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"./checkpoints/drqb-pong\", save_top_k=1,mode=\"max\", monitor=\"episode/Return\")\n",
    "\n",
    "trainer = Trainer(\n",
    "     accelerator='gpu',\n",
    "     devices=num_gpus,\n",
    "     max_epochs=10_000,\n",
    "     callbacks=[checkpoint_callback], \n",
    ")\n",
    "\n",
    "trainer.fit(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c71ec553",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = algo.q_net.to(device)\n",
    "q_net1 = algo.q_net1\n",
    "policy = algo.policy\n",
    "env = algo.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "106ae883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |     |     \n",
      "  X  |  O  |  -  \n",
      "_____|_____|_____\n",
      "     |     |     \n",
      "  X  |  -  |  -  \n",
      "_____|_____|_____\n",
      "     |     |     \n",
      "  X  |  O  |  -  \n",
      "     |     |     \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for episode in range(20):\n",
    "    env.reset()\n",
    "    for agent in env.agent_iter():\n",
    "        observation_mask, reward, done, info = env.last()\n",
    "        observation = torch.from_numpy(observation_mask[\"observation\"].reshape(-1)).unsqueeze(dim=0)\n",
    "        if agent == \"player_1\":\n",
    "            if not done:\n",
    "                action = policy(observation, env, q_net, observation_mask[\"action_mask\"], epsilon=0)\n",
    "            else:\n",
    "                action = None\n",
    "            env.step(action)\n",
    "        else:\n",
    "            if not done:\n",
    "                action = random.choice(np.argwhere(observation_mask[\"action_mask\"] ==1).reshape(-1))\n",
    "            else:\n",
    "                action = None\n",
    "            env.step(action)\n",
    "        env.render()\n",
    "        clear_output(wait=True)\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d54922e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0,1,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec807961",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[x == 0] = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f47ed5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2,  1, -2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2104d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
